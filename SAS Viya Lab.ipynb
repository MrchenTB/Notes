{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f4c74c0-5ed4-4ef7-8c2c-bf4686b12098",
   "metadata": {},
   "source": [
    "### 在使用Open source node建立模型時，用以下方式區分訓練集與驗證集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fbe3f9-aecf-4452-9d58-e81af7533862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 從 Model Studio 提供的資料框中分割訓練與驗證資料       等於1\n",
    "train = dm_inputdf[dm_inputdf[dm_partitionvar] == dm_partition_train_val]\n",
    "X_train = train.loc[:,dm_input]\n",
    "y_train = train[dm_dec_target]\n",
    "\n",
    "valid = dm_inputdf[dm_inputdf[dm_partitionvar] == 0]\n",
    "X_valid = valid.loc[:,dm_input]\n",
    "y_valid = valid[dm_dec_target]\n",
    "\n",
    "# 訓練模型時可使用\n",
    "X = dm_traindf.loc[:, dm_input]\n",
    "y = dm_traindf[dm_dec_target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6690960",
   "metadata": {},
   "source": [
    "### 計算輪廓係數的Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f22f803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "train = dm_inputdf[dm_inputdf[dm_partitionvar] == dm_partition_train_val]\n",
    "X_train = train.loc[:,dm_input]\n",
    "y_train = train[dm_dec_target]\n",
    "\n",
    "cluster_cols = ['MOBILE_P', 'REP_DATA', 'REP_MINUTES', 'REP_REP_AGE', 'TECH_PROBLEM',\n",
    "       'TOTAL_TECH_PROBLEM', 'BMRP', 'CP', 'GENDER', 'P_TYPE', 'UT', 'UV']\n",
    "\n",
    "num_cols = ['MOBILE_P', 'REP_DATA', 'REP_MINUTES', 'REP_REP_AGE', 'TECH_PROBLEM', 'TOTAL_TECH_PROBLEM', 'CP']\n",
    "cat_cols = ['BMRP', 'GENDER', 'P_TYPE']\n",
    "\n",
    "num_transformer = Pipeline([\n",
    "       ('imputer', SimpleImputer(strategy = 'mean')),\n",
    "       ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_transformer = Pipeline([\n",
    "       ('imputer', SimpleImputer(strategy = 'most_frequent')),\n",
    "       ('onehot', OneHotEncoder(sparse_output = False, handle_unknown = 'ignore'))\n",
    "])\n",
    "\n",
    "ut_uv_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')) \n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "       transformers = [\n",
    "       ('num', num_transformer, num_cols),\n",
    "       ('col', cat_transformer, cat_cols),\n",
    "       ('uv_ut', ut_uv_transformer, ['UV', 'UT'])]\n",
    ")\n",
    "\n",
    "X_transformed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "cluster_labels = train['_CLUSTER_ID_'].to_numpy()\n",
    "\n",
    "silhouette_avg = silhouette_score(X_transformed, cluster_labels)\n",
    "\n",
    "print(\"輪廓係數：\", round(silhouette_avg, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9854b0d6",
   "metadata": {},
   "source": [
    "### 特徵創建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4879c442",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "import pickle\n",
    "\n",
    "df = dm_inputdf.copy()\n",
    "df_use_data = df[dm_input].copy() # 特徵工程要用的資料\n",
    "\n",
    "# 區隔暫時不會用到的資料\n",
    "dm_idx_cols = ['CHURN', 'IU', '_dmIndex_', '_PartInd_']\n",
    "dm_idx_data = df[dm_idx_cols].copy()\n",
    "\n",
    "# 定義特徵工程函式\n",
    "def service_counts(df, UV, UT):\n",
    "    df = df.copy()\n",
    "\n",
    "    df['Service_Count'] = df[UV] + df[UT]\n",
    "    return df['Service_Count']\n",
    "\n",
    "def service_group(df, UV, UT):\n",
    "    df = df.copy()\n",
    "    df['Service_Group'] = 'no_service'\n",
    "    \n",
    "    df.loc[(df[UV] == 1) | (df[UT] == 1), 'Service_Group'] = 'one_service'\n",
    "    df.loc[(df[UV] == 1) & (df[UT] == 1), 'Service_Group'] = 'two_service' \n",
    "    \n",
    "    return df['Service_Group']\n",
    "\n",
    "def total_tech_average(df, total_tech_problem):\n",
    "    df = df.copy()\n",
    "\n",
    "    df['Total_Tech_Average'] = round(df[total_tech_problem] / 12, 2)\n",
    "    return df['Total_Tech_Average']\n",
    "\n",
    "def age_group(df, age_col, bins = [18, 25, 45, 60, 80], labels = ['青年', '青壯年', '中年', '老年']):\n",
    "    df = df.copy()\n",
    "\n",
    "    age_group = pd.cut(df[age_col], bins, labels = labels, include_lowest=True)\n",
    "    return age_group\n",
    "\n",
    "def tech_spike(df, tech_problem, total_tech_average):\n",
    "    df = df.copy()\n",
    "\n",
    "    df['Tech_Spike'] = df[tech_problem] - df[total_tech_average]\n",
    "    return df['Tech_Spike']\n",
    "\n",
    "def value_per_month(df, mobile_p, bmrp):\n",
    "    df = df.copy()\n",
    "    df['Value_Per_Month'] = round(df[mobile_p] / df[bmrp], 2)\n",
    "\n",
    "    return df['Value_Per_Month']\n",
    "    \n",
    "def dissat_index(df, cp, tech_problem, total_tech_average):\n",
    "    df = df.copy()\n",
    "\n",
    "    scaler_dissat = StandardScaler()\n",
    "    feature_to_scale = [cp, tech_problem, total_tech_average]\n",
    "    df[feature_to_scale] = scaler_dissat.fit_transform(df[feature_to_scale])\n",
    "\n",
    "    df['Dissat_Index'] = df[feature_to_scale].sum(axis = 1)\n",
    "    return df['Dissat_Index']\n",
    "\n",
    "def customer_group(n_cluster, df, cp, tech_problem, minutes):\n",
    "    df = df[[cp, tech_problem, minutes]].copy()\n",
    "    \n",
    "    scaler_customer = StandardScaler()\n",
    "    X_scaled = scaler_customer.fit_transform(df)\n",
    "\n",
    "    cluster_customer = KMeans(n_clusters = n_cluster, init = 'k-means++', n_init = 'auto')\n",
    "    df['customer_group'] = cluster_customer.fit_predict(X_scaled)\n",
    "\n",
    "    return df['customer_group']\n",
    "\n",
    "# 建立新特徵\n",
    "df_use_data['Service_Count'] = service_counts(df_use_data, 'IMP_UV', 'IMP_UT')\n",
    "df_use_data['Service_Group'] = service_group(df_use_data, 'IMP_UV', 'IMP_UT')\n",
    "df_use_data['Total_Tech_Average'] = total_tech_average(df_use_data, 'IMP_TOTAL_TECH_PROBLEM')\n",
    "df_use_data['Age_Group'] = age_group(df_use_data, 'IMP_REP_AGE')\n",
    "df_use_data['Tech_Spike'] = tech_spike(df_use_data, 'IMP_TECH_PROBLEM', 'Total_Tech_Average')\n",
    "df_use_data['Value_Per_Month'] = value_per_month(df_use_data, 'IMP_MOBILE_P', 'BMRP')\n",
    "df_use_data['Dissat_Index'] = dissat_index(df_use_data, 'IMP_CP', 'IMP_TECH_PROBLEM', 'Total_Tech_Average')\n",
    "df_use_data['Customer_Group'] = customer_group(3, df_use_data, 'IMP_CP', 'IMP_TECH_PROBLEM', 'IMP_REP_MINUTES')\n",
    "print(df_use_data['Customer_Group'])\n",
    "dm_scoreddf = pd.concat([df_use_data, dm_idx_data], axis = 1)\n",
    "\n",
    "df = df_use_data.copy()\n",
    "feature_to_scale = ['IMP_CP', 'IMP_TECH_PROBLEM', 'IMP_REP_MINUTES']\n",
    "scaler_customer = StandardScaler()\n",
    "scaler_customer.fit(df[feature_to_scale])\n",
    "\n",
    "feature_to_scale = ['IMP_CP', 'IMP_TECH_PROBLEM', 'Total_Tech_Average']\n",
    "scaler_dissat = StandardScaler()\n",
    "scaler_dissat.fit(df[feature_to_scale])\n",
    "\n",
    "feature_to_scale = ['IMP_CP', 'IMP_TECH_PROBLEM', 'IMP_REP_MINUTES']\n",
    "scaler_customer = StandardScaler()\n",
    "X_scaled = scaler_customer.fit_transform(df[feature_to_scale])\n",
    "cluster_customer = KMeans(n_clusters = 3, init = 'k-means++', n_init = 'auto').fit(X_scaled)\n",
    "\n",
    "with open(dm_pklpath, 'wb') as f:\n",
    "    pickle.dump(scaler_dissat, f)  # Dissat_Index 的 scaler\n",
    "    pickle.dump(scaler_customer, f)  # Customer_Group 的 cluster\n",
    "    pickle.dump(cluster_customer, f)\n",
    "\n",
    "print(dm_scoreddf.columns)\n",
    "print(dm_scoreddf.head())\n",
    "\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 載入序列化物件（從訓練 pickle）\n",
    "with open(settings.pickle_path + dm_pklname, 'rb') as f:\n",
    "    scaler_dissat = pickle.load(f)\n",
    "    scaler_customer = pickle.load(f)\n",
    "    cluster_customer = pickle.load(f)\n",
    "\n",
    "def score_method(BMRP, IMP_CP, IMP_GENDER, IMP_P_TYPE, IMP_UT, IMP_UV, IMP_MOBILE_P, IMP_REP_AGE, IMP_REP_DATA, IMP_REP_MINUTES, IMP_TECH_PROBLEM, IMP_TOTAL_TECH_PROBLEM):\n",
    "    \"Output: Service_Count, Service_Group, Total_Tech_Average, Age_Group, Tech_Spike, Value_Per_Month, Dissat_Index, Customer_Group, BMRP, IMP_CP, IMP_GENDER, IMP_P_TYPE, IMP_UT, IMP_UV, IMP_MOBILE_P, IMP_REP_AGE, IMP_REP_DATA, IMP_REP_MINUTES, IMP_TECH_PROBLEM, IMP_TOTAL_TECH_PROBLEM\"\n",
    "\n",
    "    record = pd.DataFrame({\n",
    "        'IMP_UV': [IMP_UV], 'IMP_UT': [IMP_UT], 'IMP_TOTAL_TECH_PROBLEM': [IMP_TOTAL_TECH_PROBLEM],\n",
    "        'IMP_REP_AGE': [IMP_REP_AGE], 'IMP_TECH_PROBLEM': [IMP_TECH_PROBLEM], 'IMP_MOBILE_P': [IMP_MOBILE_P],\n",
    "        'BMRP': [BMRP], 'IMP_CP': [IMP_CP], 'IMP_REP_MINUTES': [IMP_REP_MINUTES], 'IMP_GENDER': [IMP_GENDER],\n",
    "        'IMP_P_TYPE': [IMP_P_TYPE], 'IMP_REP_DATA': [IMP_REP_DATA]\n",
    "    })\n",
    "    \n",
    "    def service_counts(df, UV, UT):\n",
    "        df = df.copy()\n",
    "        df['Service_Count'] = df[UV] + df[UT]\n",
    "        return df['Service_Count'].iloc[0]\n",
    "    \n",
    "    def service_group(df, UV, UT):\n",
    "        df = df.copy()\n",
    "        df['Service_Group'] = 'no_service'\n",
    "        df.loc[(df[UV] == 1) | (df[UT] == 1), 'Service_Group'] = 'one_service'\n",
    "        df.loc[(df[UV] == 1) & (df[UT] == 1), 'Service_Group'] = 'two_service'\n",
    "        return df['Service_Group'].iloc[0]\n",
    "    \n",
    "    def total_tech_average(df, total_tech_problem):\n",
    "        df = df.copy()\n",
    "        df['Total_Tech_Average'] = round(df[total_tech_problem] / 12, 2)\n",
    "        return df['Total_Tech_Average'].iloc[0]\n",
    "    \n",
    "    def age_group(df, age_col, bins=[18, 25, 45, 60, 80], labels=['青年', '青壯年', '中年', '老年']):\n",
    "        df = df.copy()\n",
    "        age_group = pd.cut(df[age_col], bins, labels=labels, include_lowest=True)\n",
    "        return age_group.iloc[0]\n",
    "    \n",
    "    def tech_spike(df, tech_problem, total_tech_average):\n",
    "        df = df.copy()\n",
    "        df['Tech_Spike'] = df[tech_problem] - total_tech_average # 注意：total_tech_average 為輸入值\n",
    "        return df['Tech_Spike'].iloc[0]\n",
    "    \n",
    "    def value_per_month(df, mobile_p, bmrp):\n",
    "        df = df.copy()\n",
    "        df['Value_Per_Month'] = round(df[mobile_p] / df[bmrp], 2)\n",
    "        return df['Value_Per_Month'].iloc[0]\n",
    "    \n",
    "    def dissat_index(df, cp, tech_problem, total_tech_average):\n",
    "        df = df.copy()\n",
    "        feature_to_scale = [cp, tech_problem]\n",
    "        tech_avg_series = pd.Series([float(total_tech_average)], index=df.index, name='Total_Tech_Average')\n",
    "        df_scaled = scaler_dissat.transform(pd.concat([df[feature_to_scale], tech_avg_series], axis = 1))  # 使用訓練的 scaler，重用 total_tech_average\n",
    "        df['Dissat_Index'] = df_scaled.sum(axis=1)  # 調整以匹配訓練邏輯\n",
    "        return df['Dissat_Index'].iloc[0]\n",
    "    \n",
    "    def customer_group(n_cluster, df, cp, tech_problem, minutes):\n",
    "        df_subset = df[[cp, tech_problem, minutes]].copy()\n",
    "        X_scaled = scaler_customer.transform(df_subset)  # 使用訓練的 scaler\n",
    "        cluster_pred = cluster_customer.predict(X_scaled)\n",
    "        return int(cluster_pred[0])\n",
    "    \n",
    "    # 計算新特徵\n",
    "    service_count = service_counts(record, 'IMP_UV', 'IMP_UT')\n",
    "    service_group_val = service_group(record, 'IMP_UV', 'IMP_UT')\n",
    "    total_tech_avg = total_tech_average(record, 'IMP_TOTAL_TECH_PROBLEM')\n",
    "    age_group_val = age_group(record, 'IMP_REP_AGE')\n",
    "    tech_spike_val = tech_spike(record, 'IMP_TECH_PROBLEM', total_tech_avg)\n",
    "    value_per_month_val = value_per_month(record, 'IMP_MOBILE_P', 'BMRP')\n",
    "    dissat_index_val = dissat_index(record, 'IMP_CP', 'IMP_TECH_PROBLEM', total_tech_avg)  # 傳入 total_tech_avg\n",
    "    customer_group_val = customer_group(3, record, 'IMP_CP', 'IMP_TECH_PROBLEM', 'IMP_REP_MINUTES')\n",
    "\n",
    "    return (float(service_count), service_group_val, float(total_tech_avg), age_group_val,\n",
    "            float(tech_spike_val), float(value_per_month_val), float(dissat_index_val), int(customer_group_val),\n",
    "            record['BMRP'].iloc[0], record['IMP_CP'].iloc[0], record['IMP_GENDER'].iloc[0], record['IMP_P_TYPE'].iloc[0], record['IMP_UT'].iloc[0], record['IMP_UV'].iloc[0],\n",
    "            record['IMP_MOBILE_P'].iloc[0], record['IMP_REP_AGE'].iloc[0], record['IMP_REP_DATA'].iloc[0], record['IMP_REP_MINUTES'].iloc[0], record['IMP_TECH_PROBLEM'].iloc[0], record['IMP_TOTAL_TECH_PROBLEM'].iloc[0])\n",
    "\n",
    "#BMRP, IMP_CP, IMP_GENDER, IMP_P_TYPE, IMP_UT, IMP_UV, IMP_MOBILE_P, IMP_REP_AGE, IMP_REP_DATA, IMP_REP_MINUTES, IMP_TECH_PROBLEM, IMP_TOTAL_TECH_PROBLEM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb21e0ba",
   "metadata": {},
   "source": [
    "### 網格搜尋"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac83de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X = dm_traindf.loc[:, dm_input]\n",
    "y = dm_traindf[dm_dec_target]\n",
    "\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "rf_grid_search = GridSearchCV(estimator = rf_model, param_grid = rf_param_grid, cv = 5, n_jobs = -1, scoring='roc_auc')\n",
    "rf_grid_search.fit(X, y)\n",
    "print(\"隨機森林最佳參數組合：\", rf_grid_search.best_params_)\n",
    "print(\"最佳 ROC-AUC 分數：\", rf_grid_search.best_score_)\n",
    "\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1]\n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 步驟 3: 創建 GridSearchCV 實例\n",
    "# 'cv' 參數設定交叉驗證的折數\n",
    "# 'scoring' 參數設定評估指標，例如 'roc_auc'\n",
    "xgb_grid_search = GridSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_grid=xgb_param_grid,\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,  # 使用所有可用的核心進行運算\n",
    "    verbose=1   # 顯示進度\n",
    ")\n",
    "\n",
    "# 步驟 4: 執行網格搜尋\n",
    "# X_train 和 y_train 是你的訓練資料\n",
    "xgb_grid_search.fit(X, y)\n",
    "\n",
    "# 步驟 5: 獲取最佳參數和最佳模型\n",
    "print(\"最佳參數組合：\", xgb_grid_search.best_params_)\n",
    "print(\"最佳 ROC-AUC 分數：\", xgb_grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75d339",
   "metadata": {},
   "source": [
    "### SMOTENC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e806c050",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from imblearn.over_sampling import SMOTENC\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = dm_inputdf.copy()\n",
    "df.drop(['_dmIndex_', 'IU'], axis = 1, inplace = True)\n",
    "# 區隔暫時不會用到的資料\n",
    "total_cols = df.drop(['CHURN'], axis = 1).columns\n",
    "dm_idx_cols = ['CHURN', 'IU', '_dmIndex_', '_PartInd_']\n",
    "\n",
    "# 從 Model Studio 提供的資料框中分割訓練與驗證資料\n",
    "train = df[dm_inputdf[dm_partitionvar] == dm_partition_train_val]\n",
    "X_train = train.loc[:,dm_input]\n",
    "y_train = train[dm_dec_target]\n",
    "\n",
    "valid = dm_inputdf[dm_inputdf[dm_partitionvar] == 0]\n",
    "X_valid = valid.drop(['CHURN'], axis = 1)\n",
    "y_valid = valid[dm_dec_target]\n",
    "\n",
    "feature_names = X_train.columns\n",
    "cat_cols = [col for col in feature_names if train[col].nunique() < 5 and (col != 'Service_Count')]\n",
    "cat_col_indices = [X_train.columns.get_loc(col) for col in cat_cols]\n",
    "print(cat_cols)\n",
    "\n",
    "smote_nc = SMOTENC(categorical_features=cat_col_indices, random_state=42)\n",
    "X_resampled, y_resampled = smote_nc.fit_resample(X_train, y_train)\n",
    "\n",
    "train_df = pd.DataFrame(X_resampled, columns = total_cols)\n",
    "valid_df = pd.DataFrame(X_valid, columns = total_cols)\n",
    "\n",
    "train_df['CHURN'] = y_resampled.reset_index(drop=True)\n",
    "valid_df['CHURN'] = y_valid.reset_index(drop=True)\n",
    "\n",
    "df = pd.concat([train_df, valid_df], axis=0, ignore_index=True)\n",
    "df.fillna(1)\n",
    "\n",
    "dm_scoreddf = df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
